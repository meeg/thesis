\chapter{Vertexing Search}
The HPS vertexing search is a low-background search for heavy photons with displaced vertices.
This search is sensitive to heavy photons with low coupling, and therefore low production but long decay lengths.

The event selection for the vertexing search requires cuts in addition to the basic cuts described in \ref{sec:event_selection}.
These vertex quality cuts are tuned to suppress the tails of the vertex distribution, and are described in \ref{sec:vertex_cuts}.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=1,angle=-90]{vertexing/figs/golden_mres}
\end{center}
    \caption{Dataset for the vertexing analysis.}
    \label{fig:zvsmass}
\end{figure}

The events passing cuts are reduced to a 2-D dataset of points $(m,z)$, where each point is the mass $m$ and vertex Z-coordinate $z$ of an event: see Figure \ref{fig:zvsmass}.
The vertex $z$ is defined relative to the nominal target position; as explained in Section \ref{sec:target_z}, the actual target position is at $z_{target}\approx 5$ mm.
A rectangular window is scanned across the distribution to search for a heavy photon with $m_{A'}=m$: the width of the distribution is set by the mass resolution, and the minimum $z$ is set to remove the large background from prompt $e^+e^-$ pairs.
%This distribution is then scanned in $m$ to search for a heavy photon with $m_{A'}=m$.
%At each $m_{A'}$, a mass slice is made to restrict the dataset to the resolution-limited region where the heavy photon signal would be present, and a cut is made on $z$ to remove the large background from prompt $e^+e^-$ pairs.
%The combined effect is to select events in a rectangular region in $(m,z)$ that
This minimizes the prompt backgrounds and maximizes the heavy photon signal.

Two types of analyses are performed on the data in the window.
The first is a cut-and-count analysis, where the number of events found in the rectangular region is compared to the number expected; this analysis gives the significance of the observed excess over background, and is described in Section \ref{sec:significance}.
The second analysis uses the optimum interval method to set an upper limit on the heavy photon production rate, and is described in Section \ref{sec:limits}.

\section{Monte Carlo Samples}
All of the events of interest (tridents, heavy photons, and wide-angle bremsstrahlung) are generated using MadGraph/MadEvent version 4 \cite{alwall_madgraph/madevent_2007}.
Beam backgrounds, such as elastically scattered electrons, that are created in the target and create pileup in the detector, are simulated using EGS5 \cite{hirayama_egs5_2005} and Geant4 \cite{agostinelli_geant4simulation_2003}.

\subsection{Trident and Wide-Angle Bremsstrahlung Monte Carlo}
\label{sec:tri_mc}
Both ``full-diagram'' and radiative tridents are simulated using MadGraph/MadEvent.
The generator for full-diagram tridents includes radiative tridents, Bethe-Heitler tridents, and the interference terms.
Since radiative tridents are important for normalizing the heavy photon rate, radiative tridents are generated separately.

Wide-angle bremsstrahlung is the other important source of $e^+e^-$ pairs; as explained in \ref{sec:physics_backgrounds}, the primary electron and the positron from the pair conversion can fake an $e^+e^-$ pair.
MadGraph/MadEvent is used to produce $e^-\gamma$ events because it handles the angle correlations and the momentum transfer to the nucleus correctly.
The photon pair conversion is simulated in EGS5 (if in the target) or Geant4 (if in the tracker).

\subsection{Heavy Photon Monte Carlo}
\label{sec:ap_mc}
Heavy photon Monte Carlo is generated at values of $m_{A'}$ spaced out across the region of interest: 15, 16, 17, 18, 19, 20, 22, 24, 26, 28, 30, 35, 40, 50, 60, 70, 80, and 90 MeV.
The generator uses MadGraph/MadEvent, and fully simulates the momentum and angle spectra of the produced heavy photons.
In order to get complete coverage of $z>z_{target}$ for the purposes of the vertexing analysis, the decay vertices were displaced (in the direction of the heavy photon momentum, and accounting for variations in $\gamma$) according to an arbitrary decay length of $c\tau=1$ mm.

\section{Vertex Cuts}
\label{sec:vertex_cuts}
\begin{table}[h]
    \begin{center}
        \begin{tabular}{lc}   
            \hline \hline
            Trigger type & ``pairs-1'' trigger \\
            Track-cluster matching (position) & $\chi^2_{match}<10$ \\
            Track-cluster matching (time) & $|t_{cl}-t_{trk}-43|<4$ ns \\
            Cluster time coincidence & $|t_{cl}(e^-)-t_{cl}(e^+)|<2$ ns \\
            Top-bottom requirement & $\sign(y_{cl}(e^-))\neq\sign(y_{cl}(e^-))$ \\
            Elastics cut & $p(e^-)<0.75E_{beam}$ \\
            Momentum sum cut & $p_{tot}(e^+e^-)<1.15E_{beam}$ \\
            Radiative cut & $p_{tot}(e^+e^-)>0.8E_{beam}$ \\
            \hline \hline
            Track quality & $\chi^2_{trk}<30$ \\
            Layer 1 isolation & 0.5 \\
            Beamspot-constrained vertex quality & $\chi^2<10$ \\
            Layer 1 requirement & layer 1 hits for both tracks \\
            Momentum asymmetry & $|p(e^-)-p(e^+)|/(p(e^-)+p(e^+))<0.4$ \\
            Positron DOCA & $d_0(e^+)<1.5$ mm \\
            \hline \hline
        \end{tabular}
        \caption{The full set of event selection cuts for the vertexing analysis.
        Cuts carried over from the base selection are on top; cuts specific (or tightened) for vertexing are on bottom.}
        \label{tab:vertex_cuts} 
    \end{center}
\end{table}

Broadly speaking, there are three types of events that the vertexing cuts are meant to eliminate: layer 1 scatters, mishits, and wide-angle bremsstrahlung pairs.

Multiple scattering in layer 1 is the source of the Gaussian core of the vertex distribution, and also plays a role in the tails.
If one of the particles scatters in the first layer of the tracker, the track parameters at the vertex will be shifted.
The distribution of scattering angles is Gaussian at small angles where the scattering process is approximated by a random walk, but at large angles, the distribution is described by the Moli\`ere distribution, which approaches the power-law Rutherford scattering distribution.

Mishits happen when a particle scatters in the second (or later) layer of the tracker.
The scatter can shift the track so that a layer-1 hit from a different particle is more in line with the hits from layers 2 on; the track fit will then add the wrong hit to the track, and the track parameters at the vertex will be shifted.

The electron and positron of a wide-angle bremsstrahlung pair do not have the same origin: the electron comes from the bremsstrahlung interaction in the target, and the positron comes from a pair conversion that may happen in the target or in the tracker.
If the pair conversion happens in the tracker, the positron will not extrapolate to the target, and the reconstructed vertex may be pulled away from the target.
Because the positron is typically collinear with the photon, the effect on vertex $z$ is typically small, but these events are a identifiable component of the vertex distribution tails.

\subsection{Layer 1 Requirement}
\label{sec:layer1_cut}
The track reconstruction will find tracks with hits in any 5 out of the 6 layers.
This means that tracks can be reconstructed without layer 1 hits (as long as they have hits in all other layers).
Tracks without layer 1 hits have degraded mass and vertex resolution.
Furthermore, the tails of the vertex distribution extend to larger values of $z$.
For these reasons, it is not possible to use tracks with and without layer 1 hits as part of the same data set.

The background from wide-angle bremsstrahlung conversions is also significantly reduced by this cut.
In order to create charged tracks, the bremsstrahlung photon must convert in the target, either layer 1 sensor, or early enough in the upstream layer 2 sensor to make a hit there.
But in order to create charged tracks with layer 1 hits, the photon must convert in the target or the upstream layer 1 sensor.
The silicon sensors (0.35\% $X_0$) are significantly thicker than the portion of the target traversed by the average photon (half of 0.125\% $X_0$), so requiring that the track have a layer 1 hit cuts this background by roughly a factor of three.

The layer 1 requirement has a significant effect on the efficiency for long-lived, low-mass heavy photons.
As seen from the target, all layers of the tracker have their inner edges at $\pm 15$ mrad vertical angle from the beam plane.
As seen by a heavy photon decaying downstream of the target, layer 1 is at a significantly larger vertical angle than the others, and so the minimum $m_{A'}$ needed to hit layer 1 is larger.
Put another way, this means that the maximum $z$ for detecting a heavy photon of given $m_{A'}$ is smaller if layer 1 is required; this is shown in Figure \ref{fig:eff_z_alllayers}.
The impact of this effect is discussed in Section \ref{sec:reach_projections}.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.35\textwidth,page=7,angle=-90]{vertexing/figs/acceptance_data}
    \includegraphics[width=0.35\textwidth,page=7,angle=-90]{vertexing/figs/acceptance_alllayers_data}
\end{center}
\caption{Efficiency curves for $m_{A'}=40$ MeV. Left: requiring layer 1 hits for both tracks. Right: full HPS acceptance.}
    \label{fig:eff_z_alllayers}
\end{figure}

\subsection{Beamspot Constraint}


\subsection{Isolation Cut}
%\subsection{Track Momentum}
%\subsection{Positron DOCA}

\section{Fit Inputs}

%The events passing cuts are reduced to a 2-D dataset of points $(m,z)$, where each point is the mass $m$ and vertex Z-coordinate $z$ of an event.

To test for a heavy photon at mass $m_{A'}$ and coupling $\epsilon^2$, the signal, background, and resolutions must be modeled as inputs to the analyses.

The mass cut is set to keep only events with $|m-m_{A'}|<1.4 \sigma_m(m_{A'},z)$.
This mass window is chosen to accept a large fraction of the signal events, without accepting too many background events.
A window of $\pm1.4\sigma$ optimizes significance for high-statistics, high-background experiments where significance is proportional to $S/\sqrt{B}$.
The optimality of this window is not exact for low statistics, but is still approximately true.
The mass resolution depends on the mass and the vertex position, and is estimated using Monte Carlo: this is explained in Section \ref{sec:mres}.

After cutting on $m$, the dataset is reduced to one dimension.
An additional cut is made to keep only events with $z>z_{cut}$, rejecting the region where the background strongly dominates and there is no sensitivity to a signal.
$z_{cut}$ is chosen such that only 0.5 events are expected past $z=z_{cut}$, based on the fitted shape of the background distribution.
The amplitude of the background distribution is taken from the peak of the vertex distribution; the shape is taken from Monte Carlo as described in Section \ref{sec:tails}.

Setting limits requires knowledge of the expected signal distribution, $S(z;m_{A'},\epsilon^2)$.
Section \ref{sec:signal_shape} explains how the signal distribution is estimated.

\subsection{Estimating the Mass Resolution}
\label{sec:mres}

The mass resolution $\sigma_m$ for a $e^+e^-$ pair depends on the momentum resolutions $\sigma_{p_{e^+}},\sigma_{p_{e^-}}$ for the two particles and the resolution $\sigma_\theta$ of the opening angle.
Neglecting the electron mass, and using the small-angle approximation for the opening angle,
\begin{equation}
m=\sqrt{(1-\cos\theta)p_{e^+}p_{e^-}} \approx \frac{1}{\sqrt{2}}\theta\sqrt{p_{e^+}p_{e^-}}
\end{equation}
\begin{equation}
\sigma_m\approx \frac{1}{\sqrt{2}}\left(\theta \frac{\sqrt{p_{e^+}p_{e^-}}}{2}\left(\frac{\sigma_{p_{e^+}}}{p_{e^+}}+\frac{\sigma_{p_{e^-}}}{p_{e^-}}\right)  + \sigma_\theta\sqrt{p_{e^+}p_{e^-}} \right)
%\sigma_m\approx \frac{1}{\sqrt{2}}(\theta\sigma_{\sqrt{p_{e^+}p_{e^-}}} + \sigma_\theta\sqrt{p_{e^+}p_{e^-}})
\end{equation}
$\theta$ is the only term in this expression with a strong dependence on $m$ or $z$: it is proportional to $m$.
Because the momentum resolution is dominated by multiple scattering, the fractional momentum resolutions $\frac{\sigma_{p_{e^+}}}{p_{e^+}}$ and $\frac{\sigma_{p_{e^-}}}{p_{e^-}}$ do not depend strongly on the momentum; nor do they depend on the track angles.
The opening angle resolution is determined by the resolutions for the two track slopes, which do not depend strongly on the track slopes, so $\sigma_\theta$ is roughly constant.
The conclusion is that $\sigma_m$ is expected to depend linearly on $m$, and not at all on $z$.

Mass resolution is measured using the Monte Carlo samples of reconstructed heavy photons described in Section \ref{sec:ap_mc}.
For each $m_{A'}$, the residual between the reconstructed mass and true mass is plotted against the true $z$.
The width of the residual distribution at each $z$ is fitted with a Gaussian to get the mass resolution at that $z$, $\sigma_m(m_{A'},z)$.
The mass resolution is fitted with a first-order polynomial in $z$:
$\sigma_m(m_{A'},z) = a_0(m_{A'}) + a_1(m_{A'}) z$.
Then the polynomial coefficients are themselves fitted with first-degree polynomials in $m_{A'}$: $a_0(m_{A'}) = a_{00} + a_{01}m_{A'}$, $a_1(m_{A'}) = a_{10} + a_{11}m_{A'}$.
The result of this procedure is a model for the mass resolution: $\sigma_m(m_{A'},z) = (a_{00} + a_{01}m_{A'}) + (a_{10} + a_{11}m_{A'}) z$.

%However $\sigma_m$ increases with $z$ as shown in Figure \ref{fig:skewed_mres}, and $a_{11}$ is significantly positive.
%This seems to be an effect of a bug in the vertex fitter, which does not correctly calculate the opening angle at the vertex.
%The reconstructed mass has a systematic dependence on the opening angle in the X-Z plane, which widens the distribution of reconstructed masses as shown in Figure \ref{fig:mass_skew}.
%If this effect is subtracted out, the mass resolution becomes constant with respect to $z$, as shown in Figure \ref{fig:fixed_mres}.

%\begin{figure}[ht]
%\begin{center}
    %\includegraphics[width=0.7\textwidth,page=4,angle=-90]{vertexing/figs/acceptance_40}
%\end{center}
    %\caption{Mass resolution vs. $z$ for 40 MeV heavy photons. The resolution gets worse with increasing $z$.}
    %\label{fig:skewed_mres}
%\end{figure}

%\begin{figure}[ht]
%\begin{center}
    %\includegraphics[width=0.7\textwidth,page=5,angle=-90]{vertexing/figs/acceptance_40}
%\end{center}
    %\caption{Mass resolution for 40 MeV heavy photons decaying near $z=30$ mm. The mass residual has a systematic dependence on the opening angle in the X-Z plane.}
    %\label{fig:mass_skew}
%\end{figure}

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=5,angle=-90]{vertexing/figs/acceptance_30}
\end{center}
    \caption{Mass resolution vs. $z$ for 40 MeV heavy photons. The resolution is roughly constant with respect to $z$.}
    \label{fig:fixed_mres}
\end{figure}

The fitted values of $a_{10}$ and $a_{11}$ are consistent with 0, as expected.
After this procedure, this is the mass resolution model:
\begin{equation}
\sigma_m(m_{A'},z) = 0.000720 \mathrm{GeV} + 0.0382 m_{A'}
\end{equation}

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=12,angle=-90]{vertexing/figs/acceptance_data}
\end{center}
\caption{Mass resolution vs. $m_{A'}$. The blue marker is the M{\o}ller mass resolution in data.}
    \label{fig:mres_data}
\end{figure}

M{\o}ller scattering is one check of the mass resolution.
As explained in Section \ref{sec:mollers}, pairs of electrons from M{\o}ller scattering have a fixed invariant mass equal to the center-of-mass energy.
The width of the M{\o}ller mass distribution is therefore a useful check.
Figure \ref{fig:moller_mres} shows the M{\o}ller mass distribution in data, which has $\sigma_m=2.168$ MeV.
As shown in Figure \ref{fig:mres_data}, this is within 10\% of the heavy photon mass resolution from Monte Carlo (1.973 MeV).
The difference between data and Monte Carlo resolutions is believed to be due to the incomplete tracker alignment, for which work is still in progress.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=17,angle=-90]{vertexing/figs/mollerplots}
\end{center}
    \caption{Distribution of reconstructed invariant mass of M{\o}ller pairs, with a Crystal Ball fit showing the mass resolution.}
    \label{fig:moller_mres}
\end{figure}

\subsection{Estimating the Signal Distribution}
\label{sec:signal_shape}
The signal distribution is as follows, where each term can depend on $m_{A'}$ and $\epsilon^2$:
\begin{equation}
S(z) = (N_{A'}\epsilon_{reco}(z_{target}))\frac{e^{\frac{z_{target}-z}{\gamma c \tau}}}{\gamma c \tau}\frac{\epsilon_{reco}(z)}{\epsilon_{reco}(z_{target})}
\end{equation}
$N_{A'}$ is the number of heavy photons produced in the target.
The exponential function is the distribution of decays along $z$, and is normalized to 1.
$\epsilon_{reco}(z)$ is the efficiency to detect and reconstruct an $e^+e^-$ pair produced at a given $z$.
In principle, this distribution should be smeared by $\sigma_z$, the resolution of the vertex position: this is not done since the signal distribution varies slowly on the scale of $\sigma_z$ (which is 3-6 mm, depending on $m$).

\subsubsection{Production Rate and Radiative Fraction}

$N_{A'}\epsilon_{reco}(z_{target})$ is estimated using data and Equation \ref{eq:production}, which shows that $N_{A'}$ is linked to $\frac{\mathrm{d}N_{rad}}{\mathrm{d}m}$, the number of radiative tridents produced with masses around $m_{A'}$.
The data gives $\frac{\mathrm{d}N_{e^+e^-}}{\mathrm{d}m}\epsilon_{reco}(z_{target})$, the number of $e^+e^-$ pairs produced and detected in a mass window around $m_{A'}$; some fraction of these are radiative tridents.
The fraction is estimated using Monte Carlo.

The Monte Carlo samples described in Section \ref{sec:tri_mc} are used to calculate the normalized cross-sections for 

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.35\textwidth,page=5,angle=-90]{vertexing/figs/frac}
    \includegraphics[width=0.35\textwidth,page=6,angle=-90]{vertexing/figs/frac}
\end{center}
    \caption{Left: rates of processes producing $e^+e^-$ pairs, with the sum in black. Right: the radiative fraction, which is calculated by dividing the black histogram by the red histogram.}
    \label{fig:radfrac}
\end{figure}

\subsubsection{Decay Length}
The decay length is calculated using Equation \ref{eq:lifetime}, which gives the lifetime $\tau$ for a given $m_{A'}$ and $\epsilon^2$.
The boost $\gamma$ equals $E_{beam}/m_{A'}$ if the heavy photon takes the full beam energy, but this is not completely accurate; in reality the average boost is slightly less than the maximum.

Monte Carlo is used to get the correct distribution of decay lengths.
The decay $z$ is plotted for an MC sample of heavy photons with mass $m_{A'}$ and an arbitrary lifetime, with the requirement that the heavy photon momentum be at least $0.8E_{beam}$ (matching the analysis cut), and fit with an exponential.
The decay constant of the exponential is compared to the $\gamma c \tau$ that would be expected from $\gamma=E_{beam}/m_{A'}$; this shows that the typical $\gamma$ is roughly $0.95E_{beam}/m_{A'}$.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=1,angle=-90]{vertexing/figs/acceptance_40}
\end{center}
    \caption{Top: distribution of decay $z$ for heavy photons ($m_{A'}=40$ MeV, $c\tau=1$ mm) carrying at least 80\% of the beam momentum. Bottom: the momentum distribution for the heavy photons.}
    \label{fig:decay_z_truth}
\end{figure}

\subsubsection{Efficiency}

The efficiency $\epsilon_{reco}$ for reconstructing a heavy photon decay depends on $m_{A'}$ and the decay $z$.
The measurement of the radiative trident rate implicitly includes a factor of $\epsilon_{reco}(m_{A'},z_{target})$, so Monte Carlo is only needed to estimate $\frac{\epsilon_{reco}(m_{A'},z)}{\epsilon_{reco}(m_{A'},z_{target})}$, the efficiency falloff as a function of vertex displacement.
This assumes that any detector-based inefficiencies not represented in the Monte Carlo are independent of $z$.

The efficiency falls off with $z$ because the further downstream the decay, the larger the opening angle in the Y-Z plane necessary to hit layer 1 of the tracker.
At some cutoff value of $z$ it is no longer possible for both the electron and the positron to hit layer 1; the efficiency starts to fall off well before that cutoff.
This effect is more severe for lower $m_{A'}$, where the opening angle is smaller.

The efficiency is measured using the Monte Carlo samples of generated and reconstructed heavy photons described in Section \ref{sec:ap_mc}.
For each $m_{A'}$, the distribution of decay $z$ is filled both for generated and reconstructed heavy photons.
The ratio of the two distributions gives $\epsilon_{reco}(m_{A'},z)$.
This is scaled so it equals 1 at $z=z_target$, and is fitted with a function of the form $\frac{\epsilon_{reco}(m_{A'},z)}{\epsilon_{reco}(m_{A'},z_{target})} \approx \exp(p_3 z^3 + p_2 z^2 + p_1 z + p_0)$.
The parameters $p_3$, $p_2$, $p_1$, and $p_0$ are fit with polynomials in $m_{A'}$: $p_0$ and $p_1$ are fit with first-order polynomials, and $p_2$ and $p_3$ are fit with third-order polynomials.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=2,angle=-90]{vertexing/figs/acceptance_40}
\end{center}
    \caption{Top: distribution of decay $z$ for reconstructed heavy photons ($m_{A'}=40$ MeV, $c\tau=1$ mm). Bottom: efficiency curve $\frac{\epsilon_{reco}(m_{A'},z)}{\epsilon_{reco}(m_{A'},z_{target})}$, with fit.}
    \label{fig:eff_z}
\end{figure}

\subsection{Fitting the Background Distribution}
\label{sec:tails}
The background distribution consists of a Gaussian core and a non-Gaussian tail.
The width of the Gaussian core is set by multiple scattering and is well understood, but the tails extend much farther than the Gaussian.
For $z>z_{cut}$, the background distribution is dominated by the tails: $z_{cut}$ is typically at least $5\sigma_z$.

The background distribution is estimated from Monte Carlo.
This is necessary because if the background distribution were fit from the data, and there is actually a heavy photon signal in the data, the fit would be pulled so as to understate the size of the signal.
As in Figure \ref{fig:vertex_data-mc}, comparisons of the background distribution in Monte Carlo and data show that the Monte Carlo accurately simulates the processes that control the vertex resolution: the resolution in the Gaussian core and tails found in Monte Carlo are good fits to the data.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth]{vertexing/figs/slice-36}
\end{center}
\caption{Comparison of vertex Z distributions between data (black) and Monte Carlo (red).}
    \label{fig:vertex_data-mc}
\end{figure}

The 2-dimensional vertex distribution from Monte Carlo is scanned in $m$, using the same mass cut that is used for the analysis.

The background distribution is fit with a 4-parameter piecewise function defined from a Gaussian and an exponential.
The Gaussian is defined with the usual parameters of mean $\bar{z}$ and standard deviation $\sigma$.
The parameter $b$ defines the distance from the Gaussian's mean where the distribution transitions to the exponential.
The exponential is defined in terms of a decay length $l$, and its amplitude is fixed by the requirement that the function be continuous.
This function is similar to the ``GaussExp'' function described in \cite{cms_collaboration_search_2015}, except GaussExp fixes the exponential's decay length by requiring that the function be continuously differentiable.
\begin{equation}
B(z;\bar{z},\sigma,b,l)=
\begin{cases}
e^{-\frac{(z-\bar{z})^2}{2\sigma^2}} &\text{if } z\le\bar{z}+b,\\
e^{-\frac{b^2}{2\sigma^2} - (z-\bar{z}-b)/l}  &\text{if } z\ge\bar{z}+b
\end{cases}
\end{equation}

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=51,angle=-90]{vertexing/figs/golden_mres}
\end{center}
\caption{Vertex Z for a mass slice, in data. The transition from Gaussian to exponential is at roughly z=2. For this mass slice, $z_{cut}=26.55$; there are three events with $z>z_{cut}$.}
    \label{fig:vz_1d}
\end{figure}

The values of $b$ and $l$ found at each $m_{A'}$ are fitted to cubic polynomials in $m_{A'}$.
When estimating $B(z)$ for data, the values of $b$ and $l$ are fixed to the values found for Monte Carlo, but the Gaussian parameters are allowed to float.

\section{Finding Signal Significance}
\label{sec:significance}

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=3,angle=-90]{vertexing/figs/golden_mres_output}
\end{center}
\caption{Number of events found past $z_{cut}$, as a function of $m_{A'}$.}
    \label{fig:n_candidates}
\end{figure}

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=7,angle=-90]{vertexing/figs/golden_mres_output}
\end{center}
\caption{$p$-value for finding the observed number of events at each $m_{A'}$.}
    \label{fig:pval}
\end{figure}

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=108,angle=-90]{vertexing/figs/golden_mres}
\end{center}
\caption{Unbiased fit to estimate the background rate past $z_{cut}$ in the mass slice centered at 30 MeV.}
    \label{fig:bkg_fit}
\end{figure}

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.35\textwidth,page=1,angle=-90]{vertexing/figs/trials}
    \includegraphics[width=0.35\textwidth,page=6,angle=-90]{vertexing/figs/trials}
\end{center}
\caption{Left: distribution of the most significant $p$-values from 10000 runs of toy Monte Carlo. Right: mapping from local to global $p$-values.}
    \label{fig:trials}
\end{figure}

\section{Setting Limits}
\label{sec:limits}



\subsection{Optimum Interval Method}
The method chosen for setting limits is the ``optimum interval'' method by Yellin \cite{yellin_finding_2002}.
This method was developed for direct detection experiments, and is intended for low-statistics experiments where the signal shape is known, but the backgrounds are not fully understood and there is the possibility of an unexpected background.
A particular strength of the method is that it minimizes the influence of a background that is concentrated in one part of the data distribution.

The optimum interval method sets a one-sided upper limit on the number of events in a signal with a known distribution.
The method works by testing a proposed signal rate $\mu$ against the data with a confidence level $C$.
A change of variables is made so the events in the data are spaced according to the signal probability density (in other words, the expected signal distribution is uniform, and has total width $\mu$).
Then the program loops over every interval between two events, of width $x$ expected events and containing $n$ actual events.
The function $C_n(x,\mu)$ is the probability that all intervals containing $n$ events are narrower than this one (that is, that no interval with $n$ events has this low a ratio of actual to expected events).
The interval with largest value of $C_n(x,\mu)$ is the ``optimum interval'' that most strongly rejects the proposed signal rate.
The largest value found is called $C_{Max}$, and if it exceeds a threshold $\bar{C}_{Max}(C,\mu)$, $\mu$ is rejected with confidence level $C$.
The upper limit on $\mu$ is the value for which $C_{Max}=\bar{C}_{Max}(C,\mu)$.

The functions $C_n(x,\mu)$ and $\bar{C}_{Max}(C,\mu)$ pay the statistical penalties for picking the best interval.
Since they are not specific to the signal distribution, they are calculated using Monte Carlo and stored in lookup tables that are distributed with the software \cite{yellin_optimum_2011}.

The optimum interval method can be used with a known background; in this case, the known background density is added to the signal density.
Since the known background for HPS falls off rapidly, relatively little is to be gained from this.

A toy model was used to assess the optimum interval method for use in HPS, and the results are shown in Figure \ref{fig:optimum_interval_demo}.
The toy signal and background are both exponential distributions, but the background has a short decay length of 2, and the signal has a long decay length of 20; in units of mm, these are typical values for HPS.
10000 background events are generated; there is no signal.
A nuisance background of 100 events, with a decay length of 10, is present in the plot on the right.
The different limits (as a number of signal events) are plotted as a function of $z_{cut}$

The optimum interval method was compared against cut-and-count limits using the Feldman-Cousins method \cite{feldman_unified_1998}.
Both methods were run with and without subtracting the known background.
As expected, the optimum interval method always performs as well, or better than, cut-and-count given the same information.

Optimum interval limits are insensitive to background events at the edge of the search range.
This can be seen around $z=22$ and $z=26$ in the right-hand plot, where there are events from the nuisance background: the cut-and-count limits jump up discontinuously when $z_{cut}$ moves past the event, but the optimum interval limits vary smoothly.
It is still clear that it makes sense to avoid as much background as possible, so setting $z_{cut}$ for expected 0.5 background events is still appropriate.

The optimum interval method can be used with or without subtracting a known background.
This test shows that the performance is not dramatically better when subtracting the known background.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.35\textwidth,page=4,angle=-90]{vertexing/figs/toy_nothing}
    \includegraphics[width=0.35\textwidth,page=4,angle=-90]{vertexing/figs/toy_nosignal}
\end{center}
    \caption{Comparison of the optimum interval method with cut-and-count using Feldman-Cousins limits. The y-axis is the limit on the total signal rate as a fraction of the background rate. 
        The background (10000 events) has decay length 2, the signal has decay length 20, and the unexpected background (100 events) has decay length 5. Left plot is with only the expected background (no signal); right plot is with the unexpected background added (still no signal). The $z_{cut}$ for 0.5 expected background events is 19.8.}
    \label{fig:optimum_interval_demo}
\end{figure}

\subsection{Results}
The optimum interval method gives limits on the number of signal events past $z_{cut}$ and after acceptance and efficiency effects.
After these factors are divided out, the limits can be expressed relative to the expected production rate for a heavy photon of given $m_{A'}$ and $\epsilon^2$.
In these units, a limit of 1 or less means the heavy photon is excluded at that $m_{A'}$ and $\epsilon^2$.
The limits from this analysis are shown in Figure \ref{fig:upper_limit}, which shows that this analysis, on this data, is a factor of 106 from excluding any portion of the parameter space.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=10,angle=-90]{vertexing/figs/golden_mres_output}
\end{center}
    \caption{90\% CL upper limit on the heavy photon production rate. A value of 1 would mean exclusion; the lowest contour on this plot is 200, and the lowest point is 106.}
    \label{fig:upper_limit}
\end{figure}

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=11,angle=-90]{vertexing/figs/golden_mres_output}
\end{center}
    \caption{Number of detectable heavy photon events expected past $z_{cut}$. The highest contour is at 0.03 events.}
    \label{fig:detectable}
\end{figure}

\subsubsection{Reach Projections with Planned Improvements}
\label{sec:reach_projections}
Several factors will improve this reach in future iterations of this analysis.
First, the number of events with $z>z_{cut}$ was larger than predicted by the background fit.
Better event cuts may be able to reduce this background.
The reach with zero events past $z_{cut}$ is shown in Figure \ref{fig:upper_limit_nosignal}, where the strongest exclusion is down to 88.1.
If, after refining cuts, the background shape comes to match the exponential form used in this analysis, the reach will be very close to this zero-events reach.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=10,angle=-90]{vertexing/figs/golden_mres_nosignal_output}
\end{center}
\caption{90\% CL upper limit on the heavy photon production rate, assuming no events past $z_{cut}$. A value of 1 would mean exclusion; the lowest contour on this plot is 100, and the lowest point is 88.1.}
    \label{fig:upper_limit_nosignal}
\end{figure}


Second, this analysis only used the unblinded fraction of the 2015 data.
The full set increases statistics by a factor of 9.77, and therefore the number of detectable heavy photons.
As shown in Figure \ref{fig:upper_limit_fullset}, this improves the exclusion to 14.7.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=10,angle=-90]{vertexing/figs/golden_fullset_mres_nosignal_output}
\end{center}
\caption{90\% CL upper limit on the heavy photon production rate, projected for the full 2015 data set, and assuming no events past $z_{cut}$. A value of 1 would mean exclusion; the lowest contour on this plot is 20, and the lowest point is 14.7.}
    \label{fig:upper_limit_fullset}
\end{figure}

Finally, this analysis only used the events where both tracks made hits in layer 1 of the tracker.
As explained in Section \ref{sec:layer1_cut}
Figure \ref{fig:eff_z_alllayers} shows the difference in efficiency between the current analysis and the full acceptance.
Doing this will require tuning cuts separately for the sets of events that miss layer 1, and combining the data sets (the optimum interval method can be used for this \cite{yellin_ways_2011}).
As shown in Figure \ref{fig:upper_limit_fullset_alllayers}, this improves the exclusion to 6.82.
The mass range of the search is also substantially improved, because the lower values of $m_{A'}$ are more strongly affected by the layer 1 requirement.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=10,angle=-90]{vertexing/figs/golden_fullset_mres_allayers_nosignal_output}
\end{center}
\caption{90\% CL upper limit on the heavy photon production rate, projected for the full 2015 data set, using the full HPS acceptance, and assuming no events past $z_{cut}$. A value of 1 would mean exclusion; the lowest contour on this plot is 7, and the lowest point is 6.82.}
    \label{fig:upper_limit_fullset_alllayers}
\end{figure}
