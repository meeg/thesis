\chapter{Vertexing Search}
The HPS vertexing search is a low-background search for heavy photons with displaced vertices.
This search is sensitive to heavy photons with low coupling, and therefore low production but long decay lengths.

The pair selection for the vertexing search requires cuts in addition to the basic cuts described in \ref{sec:event_selection}.
These vertex quality cuts are tuned to suppress the tails of the vertex distribution, and are described in \ref{sec:vertex_cuts}.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=1,angle=-90]{vertexing/figs/golden_mres}
\end{center}
    \caption{Dataset for the vertexing analysis. The core of the vertex distribution is the horizontal band at $z=-5$; a heavy photon signal would appear as a band reaching upwards from the core.}
    \label{fig:zvsmass}
\end{figure}


The pairs passing cuts are reduced to a 2-D dataset of points $(m,z)$, where each point is the mass $m$ and vertex Z-coordinate $z$ of a pair: see Figure \ref{fig:zvsmass}.
The vertex $z$ is defined relative to the nominal target position; as explained in Section \ref{sec:target_z}, the actual target position is at $z_{target}\approx -5$ mm.
To test for a heavy photon with $m_{A'}=m$, a resolution-limited mass slice is taken from the 2-D data set, and the vertex $z$ of the pairs in this mass slice are plotted as in Figure \ref{fig:vz_1d}.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=51,angle=-90]{vertexing/figs/golden_mres}
\end{center}
\caption{Vertex Z for a resolution-limited mass slice (in data). A heavy photon signal would appear as a long rightwards tail (longer than the exponential fitted by the blue curve, which is the tail of the prompt vertex distribution).
    For this mass slice, $z_{cut}=26.55$; there are three events with $z>z_{cut}$.}
    \label{fig:vz_1d}
\end{figure}

A cut is made at a minimum $z>z_{cut}$ to remove the large background from prompt $e^+e^-$ pairs.
$z_{cut}$ (plotted in Figure \ref{fig:zcut}) is set so the expected number of events past $z_{cut}$ is 0.5 according to a model of the prompt pairs distribution, described in Section \ref{sec:tails}.
This minimizes the prompt backgrounds and maximizes the heavy photon signal.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=1,angle=-90]{vertexing/figs/golden_mres_output}
\end{center}
\caption{The value of the cut $z>z_{cut}$, as a function of the center of the mass slice.}
    \label{fig:zcut}
\end{figure}



Two types of analyses are performed on the data in the window.
The first is a cut-and-count analysis, where the number of pairs found in the rectangular region is compared to the number expected; this analysis gives the significance of the observed excess over background, and is described in Section \ref{sec:significance}.
The second analysis uses the optimum interval method to set an upper limit on the heavy photon production rate, and is described in Section \ref{sec:limits}.

\section{Monte Carlo Samples}
All of the $e^+e^-$ pair events of interest (tridents, heavy photons, and wide-angle bremsstrahlung) are generated using MadGraph/MadEvent version 4 \cite{alwall_madgraph/madevent_2007}.
Beam backgrounds, such as elastically scattered electrons, that are created in the target and create pileup in the detector, are simulated using EGS5 \cite{hirayama_egs5_2005} and Geant4 \cite{agostinelli_geant4simulation_2003}.

\subsection{Trident and Wide-Angle Bremsstrahlung Monte Carlo}
\label{sec:tri_mc}
Both ``full-diagram'' and radiative tridents are simulated using MadGraph/MadEvent.
The generator for full-diagram tridents includes radiative tridents, Bethe-Heitler tridents, and the interference terms.
Since radiative tridents are important for normalizing the heavy photon rate, radiative tridents are generated separately.

Wide-angle bremsstrahlung is the other important source of $e^+e^-$ pairs; as explained in \ref{sec:physics_backgrounds}, the primary electron and the positron from the pair conversion can fake an $e^+e^-$ pair.
MadGraph/MadEvent is used to produce $e^-\gamma$ events because it handles the angle correlations and the momentum transfer to the nucleus correctly.
The photon pair conversion is simulated in EGS5 (if in the target) or Geant4 (if in the tracker).

\subsection{Heavy Photon Monte Carlo}
\label{sec:ap_mc}
Heavy photon Monte Carlo is generated at values of $m_{A'}$ spaced out across the region of interest: 15, 16, 17, 18, 19, 20, 22, 24, 26, 28, 30, 35, 40, 50, 60, 70, 80, and 90 MeV.
The generator uses MadGraph/MadEvent, and fully simulates the momentum and angle spectra of the produced heavy photons.
In order to get complete coverage of $z>z_{target}$ for the purposes of the vertexing analysis, the decay vertices were displaced (in the direction of the heavy photon momentum, and accounting for variations in $\gamma$) according to an arbitrary decay length of $c\tau=1$ mm.

\section{Vertex Cuts}
\label{sec:vertex_cuts}
\begin{table}[ht]
    \begin{center}
        \begin{tabular}{lc}   
            \hline \hline
            Trigger type & ``pairs-1'' trigger \\
            Track-cluster matching (position) & $\chi^2_{match}<10$ \\
            Track-cluster matching (time) & $|t_{cl}-t_{trk}-43|<4$ ns \\
            Cluster time coincidence & $|t_{cl}(e^-)-t_{cl}(e^+)|<2$ ns \\
            Top-bottom requirement & $\sign(y_{cl}(e^-))\neq\sign(y_{cl}(e^-))$ \\
            Elastics cut & $p(e^-)<0.75E_{beam}$ \\
            Momentum sum cut & $p_{tot}(e^+e^-)<1.15E_{beam}$ \\
            Radiative cut & $p_{tot}(e^+e^-)>0.8E_{beam}$ \\
            \hline \hline
            Track quality & $\chi^2_{trk}<30$ \\
            Layer 1 requirement & layer 1 hits for both tracks \\
            Layer 1 isolation & 0.5 mm \\
            Beamspot-constrained vertex quality & $\chi^2<10$ \\
            Momentum asymmetry & $|p(e^-)-p(e^+)|/(p(e^-)+p(e^+))<0.4$ \\
            Positron DOCA & $d_0(e^+)<1.5$ mm \\
            \hline \hline
        \end{tabular}
        \caption{The full set of pair selection cuts for the vertexing analysis.
        Cuts carried over from the base selection are on top; cuts specific (or tightened) for vertexing are on bottom.}
        \label{tab:vertex_cuts} 
    \end{center}
\end{table}

Broadly speaking, there are three types of events that the vertexing cuts are meant to eliminate: layer 1 scatters, mishits, and wide-angle bremsstrahlung pairs.

Multiple scattering in layer 1 is the source of the Gaussian core of the vertex distribution, and also plays a role in the tails.
If one of the particles scatters in the first layer of the tracker, the track parameters at the vertex will be shifted.
The distribution of scattering angles is Gaussian at small angles where the scattering process is approximated by a random walk, but at large angles, the distribution is described by the Moli\`ere distribution, which approaches the power-law Rutherford scattering distribution.

Mishits happen when a particle scatters in the second (or later) layer of the tracker.
The scatter can shift the track so that a layer-1 hit from a different particle is more in line with the hits from layers 2 on; the track fit will then add the wrong hit to the track, and the track parameters at the vertex will be shifted.

The electron and positron of a wide-angle bremsstrahlung pair do not have the same origin: the electron comes from the bremsstrahlung interaction in the target, and the positron comes from a pair conversion that may happen in the target or in the tracker.
If the pair conversion happens in the tracker, the positron will not extrapolate to the target, and the reconstructed vertex may be pulled away from the target.
Because the positron is typically collinear with the photon, the effect on vertex $z$ is typically small, but these events are a identifiable component of the vertex distribution tails.

\subsection{Layer 1 Requirement}
\label{sec:layer1_cut}
The track reconstruction will find tracks with hits in any 5 out of the 6 layers.
This means that tracks can be reconstructed without layer 1 hits (as long as they have hits in all other layers).
Tracks without layer 1 hits have degraded mass and vertex resolution.
Furthermore, the tails of the vertex distribution extend to larger values of $z$.
For these reasons, it is not possible to use tracks with and without layer 1 hits as part of the same data set.

The background from wide-angle bremsstrahlung conversions is also significantly reduced by this cut.
In order to create charged tracks, the bremsstrahlung photon must convert in the target, either layer 1 sensor, or early enough in the upstream layer 2 sensor to make a hit there.
But in order to create charged tracks with layer 1 hits, the photon must convert in the target or the upstream layer 1 sensor.
The silicon sensors (0.35\% $X_0$) are significantly thicker than the portion of the target traversed by the average photon (half of 0.125\% $X_0$), so requiring that the track have a layer 1 hit cuts this background by roughly a factor of three.

The layer 1 requirement has a significant effect on the efficiency for long-lived, low-mass heavy photons.
As seen from the target, all layers of the tracker have their inner edges at $\pm 15$ mrad vertical angle from the beam plane.
As seen by a heavy photon decaying downstream of the target, layer 1 is at a significantly larger vertical angle than the others, and so the minimum $m_{A'}$ needed to hit layer 1 is larger.
Put another way, this means that the maximum $z$ for detecting a heavy photon of given $m_{A'}$ is smaller if layer 1 is required; this is shown in Figure \ref{fig:eff_z_alllayers}.
The impact of this effect is discussed in Section \ref{sec:reach_projections}.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.35\textwidth,page=7,angle=-90]{vertexing/figs/acceptance_data}
    \includegraphics[width=0.35\textwidth,page=7,angle=-90]{vertexing/figs/acceptance_alllayers_data}
\end{center}
\caption{Efficiency curves for $m_{A'}=40$ MeV. Left: requiring layer 1 hits for both tracks. Right: full HPS acceptance.}
    \label{fig:eff_z_alllayers}
\end{figure}

\subsection{Beamspot Constraint}
The beamspot constraint uses the vertex fit to identify the common situation where one track points back to the beamspot at the target as it should, but the other does not.
If the second track (due to scattering, mishits or any other effect) intersects the beam axis downstream of the target, the reconstructed vertex will be pulled downstream along the first track.
The effect is that the reconstructed $z$ will be pulled downstream of the target, but the reconstructed vertex $y$ is also pulled up or down in a way that is inconsistent with a true displaced decay.
The beamspot constraint is meant to identify these inconsistencies.

As explained in Section \ref{sec:vertex_recon}, the vertex reconstruction can use a ``beamspot-constrained'' fit where the vertex momentum is required to point back to the beamspot at $z_{target}$.
The $\chi^2$ of this vertex fit is the sum of two factors: the consistency of the vertex (how close the tracks are to intersecting each other) and the consistency with the beamspot constraint (how close the vertex is to pointing back to the beamspot).

\subsection{Isolation Cut}
The isolation cut rejects possible mishits by looking at the other hits in layer 1 of the tracker.
If a mishit on a track pulls the vertex downstream, the real hit from the particle will be close to the hit that was mistakenly associated with the track, but further away from the beam plane.
Therefore an ``isolation'' value is calculated for each of the layer 1 hits, as the distance to the closest hit in the outwards direction.
If any of the four isolations (one for each layer 1 sensor) is less than 0.5 mm, the pair is rejected.

\subsection{Momentum Asymmetry and Positron DOCA}
Two cuts are meant to reject wide-angle bremsstrahlung pairs.

Because the electron from a bremsstrahlung interaction typically carries more momentum than the photon, the electron in a WAB pair usually has higher momentum than the positron.
Heavy photons and radiative tridents have a symmetric distribution of electron and positron momentum, so a cut is used to reject pairs where the electron has much higher momentum than the positron.

If the pair conversion happens in the tracker, the positron track will curve wide of the target since the positron is roughly collinear with the photon, which does not bend in the magnetic field.
This appears in the track parameters as a positive DOCA (distance of closest approach) in the X-Z plane.
Therefore, pairs with large positive positron DOCA are rejected.

\subsection{Tuning Cuts}
Data is used to tune the cuts to keep pairs with $z$ close to $z_{target}$, and reject pairs with large positive $z$.
In general, tuning cuts on the data can introduce bias.
In this case, only the unblinded 10\% of the full dataset is being used (and will be used, even after the data is unblinded) for tuning cuts; even if a heavy photon is present in the tuning data, the number of expected events is negligible.
Also, since the cuts are not tuned for individual mass slices, any possible heavy photon signal will not be as prominent in the tuning process as it would be in the analysis.

Heavy photon Monte Carlo is used to check that none of the cuts have an unexpected adverse effect on efficiency for displaced vertices.

\section{Fit Inputs}

%The events passing cuts are reduced to a 2-D dataset of points $(m,z)$, where each point is the mass $m$ and vertex Z-coordinate $z$ of an event.

To test for a heavy photon at mass $m_{A'}$ and coupling $\epsilon^2$, the signal, background, and resolutions must be modeled as inputs to the analyses.

The mass cut is set to keep only events with $|m-m_{A'}|<1.4 \sigma_m(m_{A'},z)$.
This mass window is chosen to accept a large fraction of the signal events, without accepting too many background events.
A window of $\pm1.4\sigma$ optimizes significance for high-statistics, high-background experiments where significance is proportional to $S/\sqrt{B}$.
The optimality of this window is not exact for low statistics, but is still approximately true.
The mass resolution depends on the mass and the vertex position, and is estimated using Monte Carlo: this is explained in Section \ref{sec:mres}.

After cutting on $m$, the dataset is reduced to one dimension.
An additional cut is made to keep only events with $z>z_{cut}$, rejecting the region where the background strongly dominates and there is no sensitivity to a signal.
$z_{cut}$ is chosen such that only 0.5 events are expected past $z=z_{cut}$, based on the fitted shape of the background distribution.
The amplitude of the background distribution is taken from the peak of the vertex distribution; the shape is taken from Monte Carlo as described in Section \ref{sec:tails}.

Setting limits requires knowledge of the expected signal distribution, $S(z;m_{A'},\epsilon^2)$.
Section \ref{sec:signal_shape} explains how the signal distribution is estimated.

\subsection{Estimating the Mass Resolution}
\label{sec:mres}

The mass resolution $\sigma_m$ for a $e^+e^-$ pair depends on the momentum resolutions $\sigma_{p_{e^+}},\sigma_{p_{e^-}}$ for the two particles and the resolution $\sigma_\theta$ of the opening angle.
Neglecting the electron mass, and using the small-angle approximation for the opening angle,
\begin{equation}
m=\sqrt{(1-\cos\theta)p_{e^+}p_{e^-}} \approx \frac{1}{\sqrt{2}}\theta\sqrt{p_{e^+}p_{e^-}}
\end{equation}
\begin{equation}
\sigma_m\approx \frac{1}{\sqrt{2}}\left(\theta \frac{\sqrt{p_{e^+}p_{e^-}}}{2}\left(\frac{\sigma_{p_{e^+}}}{p_{e^+}}+\frac{\sigma_{p_{e^-}}}{p_{e^-}}\right)  + \sigma_\theta\sqrt{p_{e^+}p_{e^-}} \right)
%\sigma_m\approx \frac{1}{\sqrt{2}}(\theta\sigma_{\sqrt{p_{e^+}p_{e^-}}} + \sigma_\theta\sqrt{p_{e^+}p_{e^-}})
\end{equation}
$\theta$ is the only term in this expression with a strong dependence on $m$ or $z$: it is proportional to $m$.
Because the momentum resolution is dominated by multiple scattering, the fractional momentum resolutions $\frac{\sigma_{p_{e^+}}}{p_{e^+}}$ and $\frac{\sigma_{p_{e^-}}}{p_{e^-}}$ do not depend strongly on the momentum; nor do they depend on the track angles.
The opening angle resolution is determined by the resolutions for the two track slopes, which do not depend strongly on the track slopes, so $\sigma_\theta$ is roughly constant.
The conclusion is that $\sigma_m$ is expected to depend linearly on $m$, and not at all on $z$.

Mass resolution is measured using the Monte Carlo samples of reconstructed heavy photons described in Section \ref{sec:ap_mc}.
For each $m_{A'}$, the residual between the reconstructed mass and true mass is plotted against the true $z$.
The width of the residual distribution at each $z$ is fitted with a Gaussian to get the mass resolution at that $z$, $\sigma_m(m_{A'},z)$.
The mass resolution is fitted with a first-order polynomial in $z$:
$\sigma_m(m_{A'},z) = a_0(m_{A'}) + a_1(m_{A'}) z$.
Then the polynomial coefficients are themselves fitted with first-degree polynomials in $m_{A'}$: $a_0(m_{A'}) = a_{00} + a_{01}m_{A'}$, $a_1(m_{A'}) = a_{10} + a_{11}m_{A'}$.
The result of this procedure is a model for the mass resolution: $\sigma_m(m_{A'},z) = (a_{00} + a_{01}m_{A'}) + (a_{10} + a_{11}m_{A'}) z$.

%However $\sigma_m$ increases with $z$ as shown in Figure \ref{fig:skewed_mres}, and $a_{11}$ is significantly positive.
%This seems to be an effect of a bug in the vertex fitter, which does not correctly calculate the opening angle at the vertex.
%The reconstructed mass has a systematic dependence on the opening angle in the X-Z plane, which widens the distribution of reconstructed masses as shown in Figure \ref{fig:mass_skew}.
%If this effect is subtracted out, the mass resolution becomes constant with respect to $z$, as shown in Figure \ref{fig:fixed_mres}.

%\begin{figure}[ht]
%\begin{center}
    %\includegraphics[width=0.7\textwidth,page=4,angle=-90]{vertexing/figs/acceptance_40}
%\end{center}
    %\caption{Mass resolution vs. $z$ for 40 MeV heavy photons. The resolution gets worse with increasing $z$.}
    %\label{fig:skewed_mres}
%\end{figure}

%\begin{figure}[ht]
%\begin{center}
    %\includegraphics[width=0.7\textwidth,page=5,angle=-90]{vertexing/figs/acceptance_40}
%\end{center}
    %\caption{Mass resolution for 40 MeV heavy photons decaying near $z=30$ mm. The mass residual has a systematic dependence on the opening angle in the X-Z plane.}
    %\label{fig:mass_skew}
%\end{figure}

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=5,angle=-90]{vertexing/figs/acceptance_30}
\end{center}
    \caption{Mass resolution vs. $z$ for 40 MeV heavy photons. The resolution is roughly constant with respect to $z$.}
    \label{fig:fixed_mres}
\end{figure}

The fitted values of $a_{10}$ and $a_{11}$ are consistent with 0, as expected.
After this procedure, this is the mass resolution model:
\begin{equation}
\sigma_m(m_{A'},z) = 0.000720 \mathrm{GeV} + 0.0382 m_{A'}
\end{equation}

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=12,angle=-90]{vertexing/figs/acceptance_data}
\end{center}
\caption{Mass resolution vs. $m_{A'}$. The blue marker is the M{\o}ller mass resolution in data.}
    \label{fig:mres_data}
\end{figure}

M{\o}ller scattering is one check of the mass resolution.
As explained in Section \ref{sec:mollers}, pairs of electrons from M{\o}ller scattering have a fixed invariant mass equal to the center-of-mass energy.
The width of the M{\o}ller mass distribution is therefore a useful check.
Figure \ref{fig:moller_mres} shows the M{\o}ller mass distribution in data, which has $\sigma_m=2.168$ MeV.
As shown in Figure \ref{fig:mres_data}, this is within 10\% of the heavy photon mass resolution from Monte Carlo (1.973 MeV).
The difference between data and Monte Carlo resolutions is believed to be due to the incomplete tracker alignment, for which work is still in progress.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=17,angle=-90]{vertexing/figs/mollerplots}
\end{center}
    \caption{Distribution of reconstructed invariant mass of M{\o}ller pairs, with a Crystal Ball fit showing the mass resolution.}
    \label{fig:moller_mres}
\end{figure}

\subsection{Estimating the Signal Distribution}
\label{sec:signal_shape}
The signal distribution is as follows, where each term can depend on $m_{A'}$ and $\epsilon^2$:
\begin{equation}
S(z) = (N_{A'}\epsilon_{reco}(z_{target}))\frac{e^{\frac{z_{target}-z}{\gamma c \tau}}}{\gamma c \tau}\frac{\epsilon_{reco}(z)}{\epsilon_{reco}(z_{target})}
\end{equation}
$N_{A'}$ is the number of heavy photons produced in the target.
The exponential function is the distribution of decays along $z$, and is normalized to 1.
$\epsilon_{reco}(z)$ is the efficiency to detect and reconstruct an $e^+e^-$ pair produced at a given $z$.
In principle, this distribution should be smeared by $\sigma_z$, the resolution of the vertex position: this is not done since the signal distribution varies slowly on the scale of $\sigma_z$ (which is 3-6 mm, depending on $m$).

\subsubsection{Production Rate and Radiative Fraction}

$N_{A'}\epsilon_{reco}(z_{target})$ is estimated using data and Equation \ref{eq:production}, which shows that $N_{A'}$ is linked to $\frac{\mathrm{d}N_{rad}}{\mathrm{d}m}$, the number of radiative tridents produced with masses around $m_{A'}$.
The data gives $\frac{\mathrm{d}N_{e^+e^-}}{\mathrm{d}m}\epsilon_{reco}(z_{target})$, the number of $e^+e^-$ pairs produced and detected in a mass window around $m_{A'}$; some fraction of these are radiative tridents.
The fraction is estimated using Monte Carlo.

The Monte Carlo samples described in Section \ref{sec:tri_mc} are used to calculate the normalized cross-sections for 

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.35\textwidth,page=5,angle=-90]{vertexing/figs/frac}
    \includegraphics[width=0.35\textwidth,page=6,angle=-90]{vertexing/figs/frac}
\end{center}
    \caption{Left: rates of processes producing $e^+e^-$ pairs, with the sum in black. Right: the radiative fraction, which is calculated by dividing the black histogram by the red histogram.}
    \label{fig:radfrac}
\end{figure}

\subsubsection{Decay Length}
The decay length is calculated using Equation \ref{eq:lifetime}, which gives the lifetime $\tau$ for a given $m_{A'}$ and $\epsilon^2$.
The boost $\gamma$ equals $E_{beam}/m_{A'}$ if the heavy photon takes the full beam energy, but this is not completely accurate; in reality the average boost is slightly less than the maximum.

Monte Carlo is used to get the correct distribution of decay lengths.
The decay $z$ is plotted for an MC sample of heavy photons with mass $m_{A'}$ and an arbitrary lifetime, with the requirement that the heavy photon momentum be at least $0.8E_{beam}$ (matching the analysis cut), and fit with an exponential.
The decay constant of the exponential is compared to the $\gamma c \tau$ that would be expected from $\gamma=E_{beam}/m_{A'}$; this shows that the typical $\gamma$ is roughly $0.95E_{beam}/m_{A'}$.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=1,angle=-90]{vertexing/figs/acceptance_40}
\end{center}
    \caption{Top: distribution of decay $z$ for heavy photons ($m_{A'}=40$ MeV, $c\tau=1$ mm) carrying at least 80\% of the beam momentum. Bottom: the momentum distribution for the heavy photons.}
    \label{fig:decay_z_truth}
\end{figure}

\subsubsection{Efficiency}

The efficiency $\epsilon_{reco}$ for reconstructing a heavy photon decay depends on $m_{A'}$ and the decay $z$.
The measurement of the radiative trident rate implicitly includes a factor of $\epsilon_{reco}(m_{A'},z_{target})$, so Monte Carlo is only needed to estimate $\frac{\epsilon_{reco}(m_{A'},z)}{\epsilon_{reco}(m_{A'},z_{target})}$, the efficiency falloff as a function of vertex displacement.
This assumes that any detector-based inefficiencies not represented in the Monte Carlo are independent of $z$.

The efficiency falls off with $z$ because the further downstream the decay, the larger the opening angle in the Y-Z plane necessary to hit layer 1 of the tracker.
At some cutoff value of $z$ it is no longer possible for both the electron and the positron to hit layer 1; the efficiency starts to fall off well before that cutoff.
This effect is more severe for lower $m_{A'}$, where the opening angle is smaller.

The efficiency is measured using the Monte Carlo samples of generated and reconstructed heavy photons described in Section \ref{sec:ap_mc}.
For each $m_{A'}$, the distribution of decay $z$ is filled both for generated and reconstructed heavy photons.
The ratio of the two distributions gives $\epsilon_{reco}(m_{A'},z)$.
This is scaled so it equals 1 at $z=z_target$, and is fitted with a function of the form $\frac{\epsilon_{reco}(m_{A'},z)}{\epsilon_{reco}(m_{A'},z_{target})} \approx \exp(p_3 z^3 + p_2 z^2 + p_1 z + p_0)$.
The parameters $p_3$, $p_2$, $p_1$, and $p_0$ are fit with polynomials in $m_{A'}$: $p_0$ and $p_1$ are fit with first-order polynomials, and $p_2$ and $p_3$ are fit with third-order polynomials.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=2,angle=-90]{vertexing/figs/acceptance_40}
\end{center}
    \caption{Top: distribution of decay $z$ for reconstructed heavy photons ($m_{A'}=40$ MeV, $c\tau=1$ mm). Bottom: efficiency curve $\frac{\epsilon_{reco}(m_{A'},z)}{\epsilon_{reco}(m_{A'},z_{target})}$, with fit.}
    \label{fig:eff_z}
\end{figure}

\subsection{Fitting the Background Distribution}
\label{sec:tails}
The background distribution consists of a Gaussian core and a non-Gaussian tail.
The width of the Gaussian core is set by multiple scattering and is well understood, but the tails extend much farther than the Gaussian.
For $z>z_{cut}$, the background distribution is dominated by the tails: $z_{cut}$ is typically at least $5\sigma_z$.

The background distribution is estimated from Monte Carlo.
This is necessary because if the background distribution were fit from the data, and there is actually a heavy photon signal in the data, the fit would be pulled so as to understate the size of the signal.
As in Figure \ref{fig:vertex_data-mc}, comparisons of the background distribution in Monte Carlo and data show that the Monte Carlo accurately simulates the processes that control the vertex resolution: the resolution in the Gaussian core and tails found in Monte Carlo are good fits to the data.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth]{vertexing/figs/slice-36}
\end{center}
\caption{Comparison of vertex Z distributions between data (black) and Monte Carlo (red).}
    \label{fig:vertex_data-mc}
\end{figure}

The 2-dimensional vertex distribution from Monte Carlo is scanned in $m$, using the same mass cut that is used for the analysis.

The background distribution is fit with a 4-parameter piecewise function defined from a Gaussian and an exponential.
The Gaussian is defined with the usual parameters of mean $\bar{z}$ and standard deviation $\sigma$.
The parameter $b$ defines the distance from the Gaussian's mean where the distribution transitions to the exponential.
The exponential is defined in terms of a decay length $l$, and its amplitude is fixed by the requirement that the function be continuous.
This function is similar to the ``GaussExp'' function described in \cite{cms_collaboration_search_2015}, except GaussExp fixes the exponential's decay length by requiring that the function be continuously differentiable.
\begin{equation}
B(z;\bar{z},\sigma,b,l)=
\begin{cases}
e^{-\frac{(z-\bar{z})^2}{2\sigma^2}} &\text{if } z\le\bar{z}+b,\\
e^{-\frac{b^2}{2\sigma^2} - (z-\bar{z}-b)/l}  &\text{if } z\ge\bar{z}+b
\end{cases}
\end{equation}

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=51,angle=-90]{vertexing/figs/golden_mres}
\end{center}
\caption{Vertex Z for a mass slice in data, with the background model in blue.}
    \label{fig:vz_fit}
\end{figure}

The values of $b$ and $l$ found at each $m_{A'}$ are fitted to cubic polynomials in $m_{A'}$.
When estimating $B(z)$ for data, the values of $b$ and $l$ are fixed to the values found for Monte Carlo, but the Gaussian parameters are allowed to float.

\section{Finding Signal Significance}
\label{sec:significance}
The signal significance is a measure of the inconsistency of the data with the background-only assumption.
It is expressed in terms of a $p$-value, which is the probability under the background-only hypothesis of observing an apparent signal that is at least as significant as what was seen in the data.
By convention, the $p$-value is converted to an equivalent significance $Z$, defined such that $p$ equals the $p$-value of a $Z$-sigma upward fluctuation in a Gaussian variable.

For this analysis, the significance is calculated using $n$, the number of pairs counted past $z_{cut}$, and $b$, the number of background events expected.
$p$ equals the probability $P(n;b) = \sum^{\infty}_{k=n} \frac{b^k}{k!} e^{-b}$ of drawing at least $n$ events from a Poisson distribution with mean $b$.
%However, a value of $P(n;b)>0.5$ indicates that the observed number of events is small relative to the expected background.
%In this case a downward statistical fluctuation is assumed, and the data is considered consistent with the background-only assumption: in this case $p=0.5$, since under the background-only assumption downward fluctuations happen half the time.

According to the expected background distribution and the definition of $z_{cut}$, $b$ should equal 0.5.
However, as shown in Figure \ref{fig:n_candidates}, there appears to be an unexpected level of background that is above the 0.5 level, and varies smoothly with $m$.
This cannot be a heavy photon.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=3,angle=-90]{vertexing/figs/golden_mres_output}
\end{center}
\caption{Number of events found past $z_{cut}$, as a function of $m_{A'}$.}
    \label{fig:n_candidates}
\end{figure}

Since the background is larger than 0.5, $b$ must be approximated using the data.
$b$ for a given $m_{A'}$ hypothesis must be approximated in a way that is not biased by the data in the $m_{A'}$ mass slice.
This is done as shown in Figure \ref{fig:bkg_fit} by taking the mass slices that do not overlap with the $m_{A'}$ mass slice, and fitting the trend in $n$ as a quadratic function of $m_{A'}$.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=108,angle=-90]{vertexing/figs/golden_mres}
\end{center}
\caption{Unbiased fit to estimate the background rate past $z_{cut}$ in the mass slice centered at 30 MeV.}
    \label{fig:bkg_fit}
\end{figure}

The $p$-values from this procedure are shown in Figure \ref{fig:pval}.
The smallest $p$-value is 0.0372, but it is not correct to use this as the significance of the data set.
These are ``local'' $p$-values: the $p$-value for a mass slice represents the consistency of that slice of the data with the background-only hypothesis.
But the objective of this analysis is to find the significance of the full data set, not individual slices: how consistent is the data with the background-only hypothesis?
This is expressed by a ``global'' $p$-value.
\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=7,angle=-90]{vertexing/figs/golden_mres_output}
\end{center}
\caption{Local $p$-value for finding the observed number of events at each $m_{A'}$. The minimum value is 0.0372.}
    \label{fig:pval}
\end{figure}


The ``global'' $p$-value is found by taking the minimum of all local $p$-values, and finding the probability of finding a $p$-value at least as significant as this one.
The global $p$-value is always larger (less significant) than the local $p$-value because there are multiple local $p$-values.
This is known as the ``look-elsewhere effect.''

A simple brute-force method of finding the global $p$-value is to run a Monte Carlo simulation of the background-only hypothesis and tabulate the probability of finding a given minimum local $p$-value.
The cumulative distribution of minimum local $p$-values, scaled to 1, gives the mapping to global $p$-value.
The distribution of events past $z_{cut}$ is fit to a quadratic function, and data sets are drawn from this distribution: there are 12 events past $z_{cut}$, and so the number of events in each Monte Carlo data set is drawn from a Poisson distribution with a mean of 12.
The distribution of minimum local $p$-values is shown on the left side of Figure \ref{fig:trials}, and the cumulative distribution is on the right.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.35\textwidth,page=1,angle=-90]{vertexing/figs/trials}
    \includegraphics[width=0.35\textwidth,page=6,angle=-90]{vertexing/figs/trials}
\end{center}
\caption{Left: distribution of the most significant $p$-values from 10000 runs of toy Monte Carlo. Right: mapping from local to global $p$-values.}
    \label{fig:trials}
\end{figure}

After this procedure, the global $p$-value is found to be 0.537 for a local $p$-value of 0.0372.
This is completely consistent with the background-only hypothesis; it means that in fact the local $p$-value is less significant than the median $p$-value.

\section{Setting Limits}
\label{sec:limits}
An upper limit on the heavy photon production at a given $m_{A'}$ and $\epsilon^2$ is the maximum rate at which heavy photons could be produced, and still be consistent with the data.
The confidence level used for this analysis is 90\%: in other words, if a heavy photon signal does exist at a given rate, the limit set by this analysis will (incorrectly) exclude that signal rate only 10\% of the time.
The meaningful target for this analysis is the heavy photon production rate given by Equation \ref{eq:production}; if the upper limit at a given $m_{A'}$ and $\epsilon^2$ is below that rate, the analysis has excluded the possibility of a heavy photon at that $m_{A'}$ and $\epsilon^2$.

Upper limits do not distinguish between a lack of sensitivity (insufficient data to say anything meaningful about the presence or absence of a signal) and the presence of a signal: the upper limit will be high in either case.

\subsection{Optimum Interval Method}
The method chosen for setting limits is the ``optimum interval'' method by Yellin \cite{yellin_finding_2002}.
This method was developed for direct detection experiments, and is intended for low-statistics experiments where the signal shape is known, but the backgrounds are not fully understood and there is the possibility of an unexpected background.
A particular strength of the method is that it minimizes the influence of a background that is concentrated in one part of the data distribution.

The optimum interval method sets a one-sided upper limit on the number of events in a signal with a known distribution.
The method works by testing a proposed signal rate $\mu$ against the data with a confidence level $C$.
A change of variables is made so the events in the data are spaced according to the signal probability density (in other words, the expected signal distribution is uniform, and has total width $\mu$).
Then the program loops over every interval between two events, of width $x$ expected events and containing $n$ actual events.
The function $C_n(x,\mu)$ is the probability that all intervals containing $n$ events are narrower than this one (that is, that no interval with $n$ events has this low a ratio of actual to expected events).
The interval with largest value of $C_n(x,\mu)$ is the ``optimum interval'' that most strongly rejects the proposed signal rate.
The largest value found is called $C_{Max}$, and if it exceeds a threshold $\bar{C}_{Max}(C,\mu)$, $\mu$ is rejected with confidence level $C$.
The upper limit on $\mu$ is the value for which $C_{Max}=\bar{C}_{Max}(C,\mu)$.

The functions $C_n(x,\mu)$ and $\bar{C}_{Max}(C,\mu)$ pay the statistical penalties for picking the best interval.
Since they are not specific to the signal distribution, they are calculated using Monte Carlo and stored in lookup tables that are distributed with the software \cite{yellin_optimum_2011}.

The optimum interval method can be used with a known background; in this case, the known background density is added to the signal density.
Since the known background for HPS falls off rapidly, relatively little is to be gained from this.

A toy model was used to assess the optimum interval method for use in HPS, and the results are shown in Figure \ref{fig:optimum_interval_demo}.
The toy signal and background are both exponential distributions, but the background has a short decay length of 2, and the signal has a long decay length of 20; in units of mm, these are typical values for HPS.
10000 background events are generated; there is no signal.
A nuisance background of 100 events, with a decay length of 10, is present in the plot on the right.
The different limits (as a number of signal events) are plotted as a function of $z_{cut}$

The optimum interval method was compared against cut-and-count limits using the Feldman-Cousins method \cite{feldman_unified_1998}.
Both methods were run with and without subtracting the known background.
As expected, the optimum interval method always performs as well, or better than, cut-and-count given the same information.

Optimum interval limits are insensitive to background events at the edge of the search range.
This can be seen around $z=22$ and $z=26$ in the right-hand plot, where there are events from the nuisance background: the cut-and-count limits jump up discontinuously when $z_{cut}$ moves past the event, but the optimum interval limits vary smoothly.
It is still clear that it makes sense to avoid as much background as possible, so setting $z_{cut}$ for expected 0.5 background events is still appropriate.

The optimum interval method can be used with or without subtracting a known background.
This test shows that the performance is not dramatically better when subtracting the known background.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.35\textwidth,page=4,angle=-90]{vertexing/figs/toy_nothing}
    \includegraphics[width=0.35\textwidth,page=4,angle=-90]{vertexing/figs/toy_nosignal}
\end{center}
    \caption{Comparison of the optimum interval method with cut-and-count using Feldman-Cousins limits. The y-axis is the limit on the total signal rate as a fraction of the background rate. 
        The background (10000 events) has decay length 2, the signal has decay length 20, and the unexpected background (100 events) has decay length 5. Left plot is with only the expected background (no signal); right plot is with the unexpected background added (still no signal). The $z_{cut}$ for 0.5 expected background events is 19.8.}
    \label{fig:optimum_interval_demo}
\end{figure}

\subsection{Results}
The optimum interval method gives limits on the number of signal events past $z_{cut}$ and after acceptance and efficiency effects.
After these factors are divided out, the limits can be expressed relative to the expected production rate for a heavy photon of given $m_{A'}$ and $\epsilon^2$.
In these units, a limit of 1 or less means the heavy photon is excluded at that $m_{A'}$ and $\epsilon^2$.
The limits from this analysis are shown in Figure \ref{fig:upper_limit}, which shows that this analysis, on this data, is a factor of 106 from excluding any portion of the parameter space.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=10,angle=-90]{vertexing/figs/golden_mres_output}
\end{center}
    \caption{90\% CL upper limit on the heavy photon production rate. A value of 1 would mean exclusion; the lowest contour on this plot is 200, and the lowest point is 106.}
    \label{fig:upper_limit}
\end{figure}

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=11,angle=-90]{vertexing/figs/golden_mres_output}
\end{center}
    \caption{Number of detectable heavy photon events expected past $z_{cut}$. The highest contour is at 0.03 events.}
    \label{fig:detectable}
\end{figure}

\subsubsection{Reach Projections with Planned Improvements}
\label{sec:reach_projections}
Several factors will improve this reach in future iterations of this analysis.
First, the number of events with $z>z_{cut}$ was larger than predicted by the background fit.
Better event cuts may be able to reduce this background.
The reach with zero events past $z_{cut}$ is shown in Figure \ref{fig:upper_limit_nosignal}, where the strongest exclusion is down to 88.1.
If, after refining cuts, the background shape comes to match the exponential form used in this analysis, the reach will be very close to this zero-events reach.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=10,angle=-90]{vertexing/figs/golden_mres_nosignal_output}
\end{center}
\caption{90\% CL upper limit on the heavy photon production rate, assuming no events past $z_{cut}$. A value of 1 would mean exclusion; the lowest contour on this plot is 100, and the lowest point is 88.1.}
    \label{fig:upper_limit_nosignal}
\end{figure}


Second, this analysis only used the unblinded fraction of the 2015 data.
The full set increases statistics by a factor of 9.77, and therefore the number of detectable heavy photons.
As shown in Figure \ref{fig:upper_limit_fullset}, this improves the exclusion to 14.7.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=10,angle=-90]{vertexing/figs/golden_fullset_mres_nosignal_output}
\end{center}
\caption{90\% CL upper limit on the heavy photon production rate, projected for the full 2015 data set, and assuming no events past $z_{cut}$. A value of 1 would mean exclusion; the lowest contour on this plot is 20, and the lowest point is 14.7.}
    \label{fig:upper_limit_fullset}
\end{figure}

Finally, this analysis only used the events where both tracks made hits in layer 1 of the tracker.
As explained in Section \ref{sec:layer1_cut}
Figure \ref{fig:eff_z_alllayers} shows the difference in efficiency between the current analysis and the full acceptance.
Doing this will require tuning cuts separately for the sets of events that miss layer 1, and combining the data sets (the optimum interval method can be used for this \cite{yellin_ways_2011}).
As shown in Figure \ref{fig:upper_limit_fullset_alllayers}, this improves the exclusion to 6.82.
The mass range of the search is also substantially improved, because the lower values of $m_{A'}$ are more strongly affected by the layer 1 requirement.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=0.7\textwidth,page=10,angle=-90]{vertexing/figs/golden_fullset_mres_allayers_nosignal_output}
\end{center}
\caption{90\% CL upper limit on the heavy photon production rate, projected for the full 2015 data set, using the full HPS acceptance, and assuming no events past $z_{cut}$. A value of 1 would mean exclusion; the lowest contour on this plot is 7, and the lowest point is 6.82.}
    \label{fig:upper_limit_fullset_alllayers}
\end{figure}


\subsubsection{Reach Comparison with 2014 Proposal}
\label{sec:proposal_reach}
